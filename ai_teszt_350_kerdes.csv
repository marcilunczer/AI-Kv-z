ID,Téma,Kérdés,A,B,C,D,Helyes
1,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
2,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
3,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
4,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
5,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
6,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
7,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
8,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
9,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
10,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
11,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
12,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
13,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
14,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
15,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
16,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
17,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
18,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
19,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
20,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
21,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
22,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
23,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
24,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
25,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
26,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
27,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
28,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
29,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
30,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
31,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
32,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
33,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
34,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
35,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
36,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
37,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
38,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
39,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
40,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
41,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
42,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
43,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
44,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
45,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
46,AI Filozófia,Melyik jellemző NEM tartozik az analitikus világképhez?,Occam-borotva,A jelenség részekre bontása,Emergens tulajdonságok hangsúlyozása,Matematika alkalmazása,C
47,AI Filozófia,Kahneman System I logikája jellemzően:,"gyors, intuitív döntéshozatal","lassú, tudatos érvelés",reflexív önellenőrzés,hosszú láncú gondolkodás,A
48,AI Filozófia,Mi az AI filozófiájának központi kérdése?,Meg lehet-e érteni a világot?,Melyik GPU a legjobb?,Hogyan írjunk kódot?,Mi a legjobb könyv az AI-ról?,A
49,AI Filozófia,Mi az analitikus világkép egyik alapelve?,A részekből építkezés,Egész rendszerek működésének vizsgálata,Emergens viselkedések hangsúlyozása,Kontekstusfüggetlenség,A
50,AI Filozófia,Mi a tudományos világkép sarokköve?,Matematika használata,Önképződés,Kreatív művészet,Önigazítás hiánya,A
51,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
52,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
53,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
54,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
55,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
56,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
57,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
58,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
59,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
60,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
61,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
62,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
63,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
64,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
65,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
66,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
67,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
68,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
69,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
70,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
71,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
72,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
73,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
74,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
75,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
76,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
77,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
78,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
79,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
80,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
81,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
82,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
83,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
84,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
85,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
86,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
87,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
88,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
89,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
90,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
91,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
92,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
93,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
94,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
95,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
96,Matematikai Alapok,Mit mér a Kullback–Leibler divergencia?,Két eloszlás közti eltérést,Adatok közti euklideszi távolságot,Mátrixinverzió pontosságát,Gradiens nagyságát,A
97,Matematikai Alapok,Melyik NEM igaz a centrális határeloszlás tételre?,Korlátozott adatmennyiségnél is mindig működik,Nagy mintáknál normális eloszlást eredményez,Független véletlen változók összegére vonatkozik,Statikus eloszlás,A
98,Matematikai Alapok,Melyik loss függvény tipikus regresszióhoz?,L2-távolság,Cross-entropy,Hinge-loss,Kullback–Leibler divergencia,A
99,Matematikai Alapok,Mit állít a Bayes-tétel?,Posterior a priori és likelihood alapján,Gradient descent szabályát,Decision tree impurity-t,Ridge regularizációt,A
100,Matematikai Alapok,Melyik állítás igaz a maximum likelihood módszerre?,A paraméterértékeknél a likelihood maximumát keresi,Mindig globális minimumot talál,Nem használ veszteségfüggvényt,Kizárólag linerális modellekre igaz,A
101,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
102,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
103,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
104,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
105,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
106,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
107,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
108,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
109,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
110,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
111,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
112,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
113,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
114,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
115,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
116,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
117,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
118,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
119,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
120,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
121,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
122,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
123,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
124,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
125,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
126,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
127,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
128,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
129,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
130,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
131,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
132,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
133,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
134,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
135,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
136,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
137,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
138,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
139,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
140,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
141,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
142,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
143,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
144,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
145,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
146,Feladatvezérelt Tanítás,Melyik lépés NEM része a supervised tanulásnak?,Interferencia,Adatgyűjtés,Annotálás,Modell kiértékelés,A
147,Feladatvezérelt Tanítás,Mi a feladatvezérelt tanítás célja?,Adott címkézés reprodukálása,Modellek tesztelése,Adatok generálása,Hálózat normalizálása,A
148,Feladatvezérelt Tanítás,Melyik adatbázis-felosztás NEM tipikus supervised tanulásnál?,Production,Training,Validation,Testing,A
149,Feladatvezérelt Tanítás,Mi a túlillesztés elkerülésének egyik módja?,Regularizáció,Mindre burn-in futtatás,Teljes adathalmaz használata,Interference,A
150,Feladatvezérelt Tanítás,Milyen loss függvényt alkalmazunk klasszifikációhoz?,Cross-entropy,L2-távolság,Euclidean distance,Hinge-loss,A
151,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
152,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
153,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
154,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
155,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
156,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
157,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
158,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
159,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
160,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
161,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
162,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
163,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
164,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
165,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
166,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
167,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
168,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
169,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
170,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
171,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
172,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
173,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
174,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
175,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
176,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
177,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
178,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
179,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
180,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
181,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
182,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
183,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
184,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
185,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
186,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
187,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
188,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
189,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
190,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
191,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
192,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
193,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
194,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
195,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
196,Klasszifikáció,Mi az accuracy definíciója?,(TP+TN)/(TP+TN+FP+FN),TP/(TP+FP),TP/(TP+FN),FP/(FP+TN),A
197,Klasszifikáció,Mi a precision értelmezése?,TP/(TP+FP),TN/(TN+FP),TP/(TP+FN),FP/(FP+TN),A
198,Klasszifikáció,Mi a recall (sensitivity)?,TP/(TP+FN),TN/(TN+FP),FP/(FP+TN),FN/(FN+TP),A
199,Klasszifikáció,Melyik nem decision tree impurity-mutató?,Levenshtein-distance,Gini-index,Entropy,Misclassification rate,A
200,Klasszifikáció,Mi a random forest alapelve?,Több decision tree többségi szavazással,Egyetlen lineáris modell,Gradient descent alapú,Különálló SVM-ek,A
201,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
202,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
203,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
204,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
205,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
206,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
207,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
208,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
209,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
210,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
211,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
212,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
213,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
214,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
215,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
216,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
217,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
218,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
219,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
220,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
221,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
222,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
223,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
224,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
225,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
226,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
227,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
228,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
229,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
230,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
231,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
232,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
233,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
234,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
235,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
236,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
237,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
238,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
239,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
240,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
241,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
242,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
243,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
244,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
245,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
246,LLM & AGI,Mikor jelent meg az “Attention Is All You Need” tanulmány?,2017,2015,2019,2021,A
247,LLM & AGI,Mi az self-attention célja?,Szavak közti relevancia mérés,Hálózat súlyainak beállítása,Tanulási ráta szabályozás,Kimeneti normalizálás,A
248,LLM & AGI,Melyik NEM LLM-architektúra?,Convolutional,Encoder-only,Decoder-only,Encoder-decoder,A
249,LLM & AGI,Mennyi paraméterrel rendelkezik a GPT-3?,175 milliárd,37 milliárd,530 milliárd,671 milliárd,A
250,LLM & AGI,Mi az AGI fő jellemzője?,Új készségek elsajátítása előzetes adatok nélkül,Specializált feladatokra optimalizálás,Korlátozott nyelvi modell,Csak osztályozás,A
251,Nemlineáris regresszió,Melyik módszer nemlineáris regresszió esetén használható a gradient descent továbbfejlesztéseként?,Konjugált gradiens módszer,K-nearest neighbors,Random irányválasztás,Kvadratikus módszer,A
252,Nemlineáris regresszió,Mit jelent a 'lendület' (momentum) a gradient descent algoritmusban?,A korábbi gradient komponenst is figyelembe veszi,A loss függvény normalizálását,Az adatok standardizálását,A tanulási ráta változtatását,A
253,Nemlineáris regresszió,Melyik veszély fenyegeti a steepest descent módszert?,Beragadás lokális minimumokba,Előre definiált loss hibája,Túl gyors konvergencia,Túl kevés adat,A
254,Nemlineáris regresszió,Mi a random irányválasztás lényege?,Véletlenszerű irány kiválasztása és line search,A loss függvény lineáris közelítése,Batch-normalizáció,Regularizáció,A
255,Nemlineáris regresszió,Melyik nem lineáris regressziós módszer?,Softmax leképezés,Kvadratikus módszer,Steepest descent,Random irányválasztás,A
256,Nemlineáris regresszió,Melyik módszer nemlineáris regresszió esetén használható a gradient descent továbbfejlesztéseként?,Konjugált gradiens módszer,K-nearest neighbors,Random irányválasztás,Kvadratikus módszer,A
257,Nemlineáris regresszió,Mit jelent a 'lendület' (momentum) a gradient descent algoritmusban?,A korábbi gradient komponenst is figyelembe veszi,A loss függvény normalizálását,Az adatok standardizálását,A tanulási ráta változtatását,A
258,Nemlineáris regresszió,Melyik veszély fenyegeti a steepest descent módszert?,Beragadás lokális minimumokba,Előre definiált loss hibája,Túl gyors konvergencia,Túl kevés adat,A
259,Nemlineáris regresszió,Mi a random irányválasztás lényege?,Véletlenszerű irány kiválasztása és line search,A loss függvény lineáris közelítése,Batch-normalizáció,Regularizáció,A
260,Nemlineáris regresszió,Melyik nem lineáris regressziós módszer?,Softmax leképezés,Kvadratikus módszer,Steepest descent,Random irányválasztás,A
261,Nemlineáris regresszió,Melyik módszer nemlineáris regresszió esetén használható a gradient descent továbbfejlesztéseként?,Konjugált gradiens módszer,K-nearest neighbors,Random irányválasztás,Kvadratikus módszer,A
262,Nemlineáris regresszió,Mit jelent a 'lendület' (momentum) a gradient descent algoritmusban?,A korábbi gradient komponenst is figyelembe veszi,A loss függvény normalizálását,Az adatok standardizálását,A tanulási ráta változtatását,A
263,Nemlineáris regresszió,Melyik veszély fenyegeti a steepest descent módszert?,Beragadás lokális minimumokba,Előre definiált loss hibája,Túl gyors konvergencia,Túl kevés adat,A
264,Nemlineáris regresszió,Mi a random irányválasztás lényege?,Véletlenszerű irány kiválasztása és line search,A loss függvény lineáris közelítése,Batch-normalizáció,Regularizáció,A
265,Nemlineáris regresszió,Melyik nem lineáris regressziós módszer?,Softmax leképezés,Kvadratikus módszer,Steepest descent,Random irányválasztás,A
266,Nemlineáris regresszió,Melyik módszer nemlineáris regresszió esetén használható a gradient descent továbbfejlesztéseként?,Konjugált gradiens módszer,K-nearest neighbors,Random irányválasztás,Kvadratikus módszer,A
267,Nemlineáris regresszió,Mit jelent a 'lendület' (momentum) a gradient descent algoritmusban?,A korábbi gradient komponenst is figyelembe veszi,A loss függvény normalizálását,Az adatok standardizálását,A tanulási ráta változtatását,A
268,Nemlineáris regresszió,Melyik veszély fenyegeti a steepest descent módszert?,Beragadás lokális minimumokba,Előre definiált loss hibája,Túl gyors konvergencia,Túl kevés adat,A
269,Nemlineáris regresszió,Mi a random irányválasztás lényege?,Véletlenszerű irány kiválasztása és line search,A loss függvény lineáris közelítése,Batch-normalizáció,Regularizáció,A
270,Nemlineáris regresszió,Melyik nem lineáris regressziós módszer?,Softmax leképezés,Kvadratikus módszer,Steepest descent,Random irányválasztás,A
271,Feladat vezérelt tanítás,Mi felel meg a maximum likelihood becslésnek?,A log likelihood maximumát keresni,A minimális gradiens igényét,A k-fold cross-validationt,A softmax normalizációt,A
272,Feladat vezérelt tanítás,Milyen loss függvényt alkalmaz a presentation_new.pdf regressziós példájában?,L2 távolság,Hinge loss,Cosine similarity,Kullback-Leibler divergencia,A
273,Feladat vezérelt tanítás,"Mit használunk, ha a mátrix inverze rossz kondíciójú?",Pszeudoinverz,Batch norm,Dropout,Momentum,A
274,Feladat vezérelt tanítás,Melyik könnyíti meg a nagyméretű adathalmazból a gradient számítást?,Batch gradient descent,Full batch descent,Random forest,Decision tree,A
275,Feladat vezérelt tanítás,Mi a cél a basis fitting eljárásban?,Adott bázisfüggvények kombinálása a legjobb illeszkedéshez,A SVM margó maximalizálása,A kNN szomszédok számának kiválasztása,A Gini impurity csökkentése,A
276,Feladat vezérelt tanítás,Mi felel meg a maximum likelihood becslésnek?,A log likelihood maximumát keresni,A minimális gradiens igényét,A k-fold cross-validationt,A softmax normalizációt,A
277,Feladat vezérelt tanítás,Milyen loss függvényt alkalmaz a presentation_new.pdf regressziós példájában?,L2 távolság,Hinge loss,Cosine similarity,Kullback-Leibler divergencia,A
278,Feladat vezérelt tanítás,"Mit használunk, ha a mátrix inverze rossz kondíciójú?",Pszeudoinverz,Batch norm,Dropout,Momentum,A
279,Feladat vezérelt tanítás,Melyik könnyíti meg a nagyméretű adathalmazból a gradient számítást?,Batch gradient descent,Full batch descent,Random forest,Decision tree,A
280,Feladat vezérelt tanítás,Mi a cél a basis fitting eljárásban?,Adott bázisfüggvények kombinálása a legjobb illeszkedéshez,A SVM margó maximalizálása,A kNN szomszédok számának kiválasztása,A Gini impurity csökkentése,A
281,Feladat vezérelt tanítás,Mi felel meg a maximum likelihood becslésnek?,A log likelihood maximumát keresni,A minimális gradiens igényét,A k-fold cross-validationt,A softmax normalizációt,A
282,Feladat vezérelt tanítás,Milyen loss függvényt alkalmaz a presentation_new.pdf regressziós példájában?,L2 távolság,Hinge loss,Cosine similarity,Kullback-Leibler divergencia,A
283,Feladat vezérelt tanítás,"Mit használunk, ha a mátrix inverze rossz kondíciójú?",Pszeudoinverz,Batch norm,Dropout,Momentum,A
284,Feladat vezérelt tanítás,Melyik könnyíti meg a nagyméretű adathalmazból a gradient számítást?,Batch gradient descent,Full batch descent,Random forest,Decision tree,A
285,Feladat vezérelt tanítás,Mi a cél a basis fitting eljárásban?,Adott bázisfüggvények kombinálása a legjobb illeszkedéshez,A SVM margó maximalizálása,A kNN szomszédok számának kiválasztása,A Gini impurity csökkentése,A
286,Feladat vezérelt tanítás,Mi felel meg a maximum likelihood becslésnek?,A log likelihood maximumát keresni,A minimális gradiens igényét,A k-fold cross-validationt,A softmax normalizációt,A
287,Feladat vezérelt tanítás,Milyen loss függvényt alkalmaz a presentation_new.pdf regressziós példájában?,L2 távolság,Hinge loss,Cosine similarity,Kullback-Leibler divergencia,A
288,Feladat vezérelt tanítás,"Mit használunk, ha a mátrix inverze rossz kondíciójú?",Pszeudoinverz,Batch norm,Dropout,Momentum,A
289,Feladat vezérelt tanítás,Melyik könnyíti meg a nagyméretű adathalmazból a gradient számítást?,Batch gradient descent,Full batch descent,Random forest,Decision tree,A
290,Feladat vezérelt tanítás,Mi a cél a basis fitting eljárásban?,Adott bázisfüggvények kombinálása a legjobb illeszkedéshez,A SVM margó maximalizálása,A kNN szomszédok számának kiválasztása,A Gini impurity csökkentése,A
291,Klasszifikáció,Melyik mutató nem szerepel a konfúziós mátrix elemzésekor?,Cross-entropy loss,Precision,Recall,Accuracy,A
292,Klasszifikáció,Mi a Gini impurity definíciója decision tree esetén?,1 - ∑ p_k^2,∑ |y - y'|,-∑ p_i log p_i,∑ (y_i - y'_i)^2,A
293,Klasszifikáció,Hogyan működik a k-nearest neighbor módszer?,A legközelebbi k pont osztályának többsége dönt,A margin maximalizálása,A softmax normalizálása,A bootstrapping,A
294,Klasszifikáció,Melyik nem jellemző supervised klasszifikáció esetén?,Unsupervised clustering,Training set,Validation set,Testing set,A
295,Klasszifikáció,Mi a k-fold cross validation lényege?,Az adatok k részre osztása és rotációja a teszteléshez,A mátrix inverzének becslése,A gradient descent gyorsítása,A loss függvény újradefiniálása,A
296,Klasszifikáció,Melyik mutató nem szerepel a konfúziós mátrix elemzésekor?,Cross-entropy loss,Precision,Recall,Accuracy,A
297,Klasszifikáció,Mi a Gini impurity definíciója decision tree esetén?,1 - ∑ p_k^2,∑ |y - y'|,-∑ p_i log p_i,∑ (y_i - y'_i)^2,A
298,Klasszifikáció,Hogyan működik a k-nearest neighbor módszer?,A legközelebbi k pont osztályának többsége dönt,A margin maximalizálása,A softmax normalizálása,A bootstrapping,A
299,Klasszifikáció,Melyik nem jellemző supervised klasszifikáció esetén?,Unsupervised clustering,Training set,Validation set,Testing set,A
300,Klasszifikáció,Mi a k-fold cross validation lényege?,Az adatok k részre osztása és rotációja a teszteléshez,A mátrix inverzének becslése,A gradient descent gyorsítása,A loss függvény újradefiniálása,A
301,Klasszifikáció,Melyik mutató nem szerepel a konfúziós mátrix elemzésekor?,Cross-entropy loss,Precision,Recall,Accuracy,A
302,Klasszifikáció,Mi a Gini impurity definíciója decision tree esetén?,1 - ∑ p_k^2,∑ |y - y'|,-∑ p_i log p_i,∑ (y_i - y'_i)^2,A
303,Klasszifikáció,Hogyan működik a k-nearest neighbor módszer?,A legközelebbi k pont osztályának többsége dönt,A margin maximalizálása,A softmax normalizálása,A bootstrapping,A
304,Klasszifikáció,Melyik nem jellemző supervised klasszifikáció esetén?,Unsupervised clustering,Training set,Validation set,Testing set,A
305,Klasszifikáció,Mi a k-fold cross validation lényege?,Az adatok k részre osztása és rotációja a teszteléshez,A mátrix inverzének becslése,A gradient descent gyorsítása,A loss függvény újradefiniálása,A
306,Klasszifikáció,Melyik mutató nem szerepel a konfúziós mátrix elemzésekor?,Cross-entropy loss,Precision,Recall,Accuracy,A
307,Klasszifikáció,Mi a Gini impurity definíciója decision tree esetén?,1 - ∑ p_k^2,∑ |y - y'|,-∑ p_i log p_i,∑ (y_i - y'_i)^2,A
308,Klasszifikáció,Hogyan működik a k-nearest neighbor módszer?,A legközelebbi k pont osztályának többsége dönt,A margin maximalizálása,A softmax normalizálása,A bootstrapping,A
309,Klasszifikáció,Melyik nem jellemző supervised klasszifikáció esetén?,Unsupervised clustering,Training set,Validation set,Testing set,A
310,Klasszifikáció,Mi a k-fold cross validation lényege?,Az adatok k részre osztása és rotációja a teszteléshez,A mátrix inverzének becslése,A gradient descent gyorsítása,A loss függvény újradefiniálása,A
311,Deep Neural Networks,Mi a Universal Approximation Theorem lényege a DNN-ekkel kapcsolatban?,Bármely folytonos függvény előállítható egy megfelelő hálózattal,A hálózatok mindig lineárisak,A DNN-ek nem tudnak nemlineáris mappinget,A mélység nem számít,A
312,Deep Neural Networks,Melyik aktívációs függvény nemlineáris?,ReLU,Mx+b,Softmax dimenzió,Covariance matrix,A
313,Deep Neural Networks,Mi biztosítja a gradiens stabilitását a backpropagation során?,Gradient clipping,Loss normalization,Momentum,Decision tree,A
314,Deep Neural Networks,Mire szolgál a batch normalization?,A belső covariancia shift csökkentésére,A dropout növelésére,A margin maximalizálására,A kNN távolság meghatározására,A
315,Deep Neural Networks,Melyik nem felsorolt regulárizációs technika?,Fisher loss,L1 regularizáció,L2 regularizáció,Dropout,A
316,Deep Neural Networks,Mi a Universal Approximation Theorem lényege a DNN-ekkel kapcsolatban?,Bármely folytonos függvény előállítható egy megfelelő hálózattal,A hálózatok mindig lineárisak,A DNN-ek nem tudnak nemlineáris mappinget,A mélység nem számít,A
317,Deep Neural Networks,Melyik aktívációs függvény nemlineáris?,ReLU,Mx+b,Softmax dimenzió,Covariance matrix,A
318,Deep Neural Networks,Mi biztosítja a gradiens stabilitását a backpropagation során?,Gradient clipping,Loss normalization,Momentum,Decision tree,A
319,Deep Neural Networks,Mire szolgál a batch normalization?,A belső covariancia shift csökkentésére,A dropout növelésére,A margin maximalizálására,A kNN távolság meghatározására,A
320,Deep Neural Networks,Melyik nem felsorolt regulárizációs technika?,Fisher loss,L1 regularizáció,L2 regularizáció,Dropout,A
321,Deep Neural Networks,Mi a Universal Approximation Theorem lényege a DNN-ekkel kapcsolatban?,Bármely folytonos függvény előállítható egy megfelelő hálózattal,A hálózatok mindig lineárisak,A DNN-ek nem tudnak nemlineáris mappinget,A mélység nem számít,A
322,Deep Neural Networks,Melyik aktívációs függvény nemlineáris?,ReLU,Mx+b,Softmax dimenzió,Covariance matrix,A
323,Deep Neural Networks,Mi biztosítja a gradiens stabilitását a backpropagation során?,Gradient clipping,Loss normalization,Momentum,Decision tree,A
324,Deep Neural Networks,Mire szolgál a batch normalization?,A belső covariancia shift csökkentésére,A dropout növelésére,A margin maximalizálására,A kNN távolság meghatározására,A
325,Deep Neural Networks,Melyik nem felsorolt regulárizációs technika?,Fisher loss,L1 regularizáció,L2 regularizáció,Dropout,A
326,Deep Neural Networks,Mi a Universal Approximation Theorem lényege a DNN-ekkel kapcsolatban?,Bármely folytonos függvény előállítható egy megfelelő hálózattal,A hálózatok mindig lineárisak,A DNN-ek nem tudnak nemlineáris mappinget,A mélység nem számít,A
327,Deep Neural Networks,Melyik aktívációs függvény nemlineáris?,ReLU,Mx+b,Softmax dimenzió,Covariance matrix,A
328,Deep Neural Networks,Mi biztosítja a gradiens stabilitását a backpropagation során?,Gradient clipping,Loss normalization,Momentum,Decision tree,A
329,Deep Neural Networks,Mire szolgál a batch normalization?,A belső covariancia shift csökkentésére,A dropout növelésére,A margin maximalizálására,A kNN távolság meghatározására,A
330,Deep Neural Networks,Melyik nem felsorolt regulárizációs technika?,Fisher loss,L1 regularizáció,L2 regularizáció,Dropout,A
